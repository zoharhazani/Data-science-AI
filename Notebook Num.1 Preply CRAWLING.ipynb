{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5d7cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deea492",
   "metadata": {},
   "source": [
    "## The crawling phase:\n",
    "## We did the crawling on the english's teachers on Preply web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb0971b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this arr we get the links of specific teacher\n",
    "urls = []\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aa8b9a",
   "metadata": {},
   "source": [
    "## we download all the pages that contains cards of English's teachers \n",
    "## each page contains 10 cards,and every card represents specific english teacher\n",
    "## we append every card to \"urls\" array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b67b2e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "student_count = []\n",
    "for i in range(477,487):\n",
    "    url = f'https://preply.com/en/online/english-tutors?page={i}'\n",
    "    res = requests.get(url,headers=headers)\n",
    "    soup = BeautifulSoup(res.content,'html.parser')\n",
    "    all_cards = soup.find_all('li',attrs={\"class\":\"TutorCardWrapper___2SCl8\"})\n",
    "    for card in all_cards:\n",
    "        href = card.find('a')['href']\n",
    "        urls.append(href)\n",
    "        try:\n",
    "            reviews.append(card.find('div',attrs={\"class\":\"ReviewsNumber___1enrU\"}).text)\n",
    "        except:\n",
    "            reviews.append(0)\n",
    "        try:\n",
    "            student_counts = card.find('span',attrs={\"class\":\"ActiveStudents___I9k62\"}).text\n",
    "            student_count.append(int(student_counts.split(' ')[0]))\n",
    "        except:\n",
    "            student_count.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b492b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b23757",
   "metadata": {},
   "source": [
    "## here we created the columns which represntes the features of the the teacher on our data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb1fc79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "  to_df = {\n",
    "    \"name\":[],\n",
    "    \"where_from\":[],\n",
    "    \"description\":[],\n",
    "    \"price\":[],\n",
    "    \"teaches\":[],\n",
    "    \"lessons\":[],\n",
    "    \"rating\":[],\n",
    "    \"speaks\":[],\n",
    "    \"more_info\":[],\n",
    "    \"popular\":[],\n",
    "    \"newly_joined\":[],\n",
    "    \"responds\":[],\n",
    "    \"has_diploma\":[],\n",
    "    \"approved_matirials\":[],\n",
    "    \"reviews\":reviews,\n",
    "    \"students_count\":student_count\n",
    "}\n",
    "t_soup = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d6140",
   "metadata": {},
   "source": [
    "## Crawling to urls:\n",
    "### after we got the information of every teacher on preply web, We inserted them to our data farme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5975df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# range(i) ---> how many teachers from urls[]\n",
    "for u in range(100):\n",
    "    speaks = []\n",
    "    teaches = []\n",
    "    res = requests.get(urls[u],headers=headers)\n",
    "    \n",
    "    t_soup = BeautifulSoup(res.content,'html.parser')\n",
    "    \n",
    "    name = t_soup.find('h2',attrs={\"class\":'name___10LsT'}).text\n",
    "    where_from=t_soup.find('img',attrs={\"class\":'flag___26DQj'})['alt']\n",
    "    description = t_soup.find('h1',attrs={\"class\":'infoRow___dv67Z headline___3KNBD'}).text\n",
    "    \n",
    "    teaches_arr = t_soup.find_all('li',attrs={\"class\":'item___2xHv5'})\n",
    "    \n",
    "    newly_joined = 0\n",
    "    has_diploma=0\n",
    "    approved_matirials=0\n",
    "    \n",
    "    for l in teaches_arr:\n",
    "        teaches.append(l.find('a').text)\n",
    "\n",
    "    speaks_arr = t_soup.find_all('li',attrs={\"class\":'item___18Wix'})\n",
    "    \n",
    "    for s in speaks_arr:\n",
    "        speaks.append(s.text)\n",
    "\n",
    "\n",
    "    try:\n",
    "        rating = t_soup.find('div',attrs={\"class\":'RatingIndicatorRating___374zP'}).text\n",
    "    except:\n",
    "        rating = 0\n",
    "        \n",
    "    try:\n",
    "        popular = t_soup.find('p',attrs={\"class\":'flexColumn___596kL'}).find('span',attrs={\"class\":\"bold___14vuF\"}).text\n",
    "    except:\n",
    "        popular = 0\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        t_soup.find('div',attrs={\"class\":\"NewTutorBadge___sVfwo\"}).text\n",
    "        newly_joined=1\n",
    "    except:\n",
    "        newly_joined = 0\n",
    "        \n",
    "    try:   \n",
    "        lessons = t_soup.find('span',attrs={\"class\":\"totalLessons___2fSGl\"}).text\n",
    "    except:\n",
    "        lessons=0\n",
    "        \n",
    "        \n",
    "    price = t_soup.find('div',attrs={\"class\":'PriceIndicatorValue___1kV4p'}).text\n",
    "    \n",
    "    try:\n",
    "        more_info = t_soup.find_all('div',attrs={\"class\":'MoreInfoItem___2Tprx'})\n",
    "        to_df['more_info'].append(more_info[1].text)   \n",
    "    except:\n",
    "        to_df['more_info'].append(np.nan)   \n",
    "\n",
    "        \n",
    "    try:\n",
    "        more_info = t_soup.find_all('div',attrs={\"class\":'MoreInfoItem___2Tprx'})\n",
    "        to_df['responds'].append(more_info[2].text)   \n",
    "    except:\n",
    "        to_df['responds'].append(np.nan) \n",
    "        \n",
    "    try:\n",
    "        t_soup.find('span',attrs={\"class\":\"diploma___1yB6Y\"}).text\n",
    "        has_diploma=1\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        has_diploma=0\n",
    "        \n",
    "    \n",
    "    try:\n",
    "        t_soup.find('b',attrs={\"class\":\"curriculumBold___3mLEI\"}).text\n",
    "        approved_matirials=1\n",
    "        \n",
    "    except:\n",
    "        approved_matirials=0\n",
    "    \n",
    "    sp = ','.join(speaks)\n",
    "    tc = ','.join(teaches)\n",
    "    to_df['name'].append(name)\n",
    "    to_df['where_from'].append(where_from)\n",
    "    to_df['description'].append(description)    \n",
    "    to_df['teaches'].append(tc)\n",
    "    to_df['speaks'].append(sp)    \n",
    "    to_df['lessons'].append(lessons)    \n",
    "    to_df['price'].append(price)\n",
    "    to_df['popular'].append(popular)\n",
    "    to_df['rating'].append(rating) \n",
    "    to_df['newly_joined'].append(newly_joined)\n",
    "    to_df['has_diploma'].append(has_diploma)\n",
    "    to_df['approved_matirials'].append(approved_matirials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc493fc",
   "metadata": {},
   "source": [
    "### As you can see we had a lot a \"try and except\" because the pages were uneven.\n",
    "#### for example: \n",
    "1. some people didnt have rating, as a result they didnt have the class that contains the rating information as other people that has that.\n",
    "2. there are some people that called \"popular\" because their features.. so they have on their card a script that says that. people who are not popular do not have it.\n",
    "3. people that just joind preply, have on their cards a bold script that says that, other people don't.\n",
    "4. there some people that wrote on their card \"More Info\", and other people mdont..\n",
    "the same happens on the \"has_diploma\",\"approved_matirials\",\"responds\" and \"lessons\" categories and respectively their classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09455a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(to_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4ae3ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf00cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('preply17.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434eb59c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
